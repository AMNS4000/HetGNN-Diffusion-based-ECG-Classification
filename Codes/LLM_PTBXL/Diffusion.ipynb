{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb\n",
    "import seaborn as sns\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(df, sampling_rate, path):\n",
    "    if sampling_rate == 100:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_lr]\n",
    "    else:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_hr]\n",
    "    data = np.array([signal for signal, meta in data])\n",
    "    return data\n",
    "\n",
    "path = '/home/naman21266/ptbxl_dataset/'\n",
    "sampling_rate=100\n",
    "\n",
    "# load and convert annotation data\n",
    "Y = pd.read_csv(path+'ptbxl_database.csv', index_col='ecg_id')\n",
    "Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# Load raw signal data\n",
    "X = load_raw_data(Y, sampling_rate, path)\n",
    "\n",
    "# Load scp_statements.csv for diagnostic aggregation\n",
    "agg_df = pd.read_csv(path+'scp_statements.csv', index_col=0)\n",
    "agg_df = agg_df[agg_df.diagnostic == 1]\n",
    "\n",
    "def aggregate_diagnostic(y_dic):\n",
    "    tmp = []\n",
    "    for key in y_dic.keys():\n",
    "        if key in agg_df.index:\n",
    "            tmp.append(agg_df.loc[key].diagnostic_class)\n",
    "    return list(set(tmp))\n",
    "\n",
    "# Apply diagnostic superclass\n",
    "Y['diagnostic_superclass'] = Y.scp_codes.apply(aggregate_diagnostic)\n",
    "\n",
    "# Split data into train and test\n",
    "test_fold = 10\n",
    "# Train\n",
    "X_train = X[np.where(Y.strat_fold != test_fold)]\n",
    "y_train = Y[(Y.strat_fold != test_fold)].diagnostic_superclass\n",
    "# Test\n",
    "X_test = X[np.where(Y.strat_fold == test_fold)]\n",
    "y_test = Y[Y.strat_fold == test_fold].diagnostic_superclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class Simple1DUnet(nn.Module):\n",
    "    def __init__(self, dim, dim_mults=(1, 2, 4, 8), channels=12):\n",
    "        super().__init__()\n",
    "        dims = [channels, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(dim),\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            self.downs.append(nn.ModuleList([\n",
    "                nn.Conv1d(dim_in, dim_out, 3, padding=1),\n",
    "                nn.GELU(),\n",
    "                nn.Conv1d(dim_out, dim_out, 3, padding=1),\n",
    "                nn.GELU(),\n",
    "                nn.Conv1d(dim_out, dim_out, 3, padding=1, stride=2)\n",
    "            ]))\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            self.ups.append(nn.ModuleList([\n",
    "                nn.Conv1d(dim_out * 2, dim_in, 3, padding=1),\n",
    "                nn.GELU(),\n",
    "                nn.Conv1d(dim_in, dim_in, 3, padding=1),\n",
    "                nn.GELU(),\n",
    "                nn.ConvTranspose1d(dim_in, dim_in, 4, stride=2, padding=1)\n",
    "            ]))\n",
    "\n",
    "        self.final_conv = nn.Conv1d(dim, channels, 1)\n",
    "\n",
    "    def forward(self, x, time):\n",
    "        t = self.time_mlp(time)\n",
    "\n",
    "        h = []\n",
    "        for conv1, gelu1, conv2, gelu2, downsample in self.downs:\n",
    "            x = conv1(x) + t\n",
    "            x = gelu1(x)\n",
    "            x = conv2(x) + t\n",
    "            x = gelu2(x)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        for conv1, gelu1, conv2, gelu2, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = conv1(x) + t\n",
    "            x = gelu1(x)\n",
    "            x = conv2(x) + t\n",
    "            x = gelu2(x)\n",
    "            x = upsample(x)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion1D(nn.Module):\n",
    "    def __init__(self, model, timesteps=1000):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "        betas = torch.linspace(0.0001, 0.02, timesteps)\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1.0 / alphas_cumprod))\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        noise = torch.randn_like(x_start) if noise is None else noise\n",
    "        return (\n",
    "            self.sqrt_recip_alphas_cumprod[t][:, None, None] * x_start +\n",
    "            (1. - self.sqrt_recip_alphas_cumprod[t][:, None, None]) * noise\n",
    "        )\n",
    "\n",
    "    def forward(self, x_start, t, noise=None):\n",
    "        x_noisy = self.q_sample(x_start, t, noise)\n",
    "        predicted_noise = self.model(x_noisy, t)\n",
    "        return predicted_noise, x_noisy\n",
    "\n",
    "def p_losses(denoise_model, x_start, t, noise=None):\n",
    "    noise = torch.randn_like(x_start) if noise is None else noise\n",
    "    predicted_noise, x_noisy = denoise_model(x_start, t, noise)\n",
    "    loss = F.mse_loss(predicted_noise, noise)\n",
    "    return loss\n",
    "\n",
    "def train_diffusion_model(model, data_loader, epochs=50):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for x_batch in data_loader:\n",
    "            model.train()\n",
    "            t = torch.randint(0, model.timesteps, (x_batch.shape[0],)).long().to(x_batch.device)\n",
    "            loss = p_losses(model, x_batch, t)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
